# -*- coding: utf-8 -*-
"""Clustering_nick

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y6doZfiDOB3x5NUdNLk9Jnn_k6pJ3kfW

**Complete your Clustering Project using the State Assigned to you.  Ensure you use MarkDown to document you work and discoveries in the process of clustering the data.**

**Libraries and references**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.neighbors import NearestNeighbors
#https://www.reneshbedre.com/blog/dbscan-python.html

"""**Mount to Google Drive**

**Load the NHTSA Data from the provided file as in previous projects**
"""

## New Code

# File path to the uploaded Excel file in Colab's /content directory
file_path = '/content/Region 4 Data.xlsx'

# Load the Excel file
accident_data_original = pd.read_excel(file_path)  # Keep original DataFrame intact

# Display the first few rows of the DataFrame
accident_data_original.head()

"""**Filter the data for your assigned state.**

**Unsderstand the size and characteristics of your data.**
"""

states = ["Missouri", "Oklahoma", "Arkansas", "Kansas"]
df = accident_data_original.loc[accident_data_original['STATENAME'].isin(states)] # Use original DataFrame
df

#Average cases per month
cases_month=pd.pivot_table(df,values='ST_CASE',index='MONTHNAME',aggfunc=pd.Series.nunique)
cases_month


#pd.pivot_table(df, values='col1', index='col2', columns='col3',aggfunc=pd.Series.nunique)

"""**Standardize the Latitude and Longitude data for analysis.**"""

df['HARM_EV_ZSCORE']=stats.zscore(df['HARM_EV'])
df['HARM_EV_ZSCORE']

df['REL_ROAD_ZSCORE']=stats.zscore(df['REL_ROAD'])
df['REL_ROAD_ZSCORE']

"""**Plot the standardized locations of accidents.**"""

plt.figure(figsize=(8,8))
plt.scatter(df['HARM_EV_ZSCORE'],df['REL_ROAD_ZSCORE'])
plt.xlabel('HARM_EV')
plt.ylabel('REL_ROAD')
plt.show()

"""**Create the Lat and Long Dataframe for use in clustering.**"""

df_lat_long=df[['HARM_EV_ZSCORE','REL_ROAD_ZSCORE']].copy()
df_lat_long

"""**This section will use K-Nearest Negihbors for help define the parameters in the DBSCAN Clustering alogrithm.  Vary the number of n_neighbors to discover the "elbow" value where the K-nn distance will be used.**"""

# n_neighbors = 7 as kneighbors function returns distance of point to itself (i.e. first column will be zeros)
nbrs = NearestNeighbors(n_neighbors=7).fit(df_lat_long)
# Find the k-neighbors of a point
neigh_dist, neigh_ind = nbrs.kneighbors(df_lat_long)
# sort the neighbor distances (lengths to points) in ascending order
# axis = 0 represents sort along first axis i.e. sort along row
sort_neigh_dist = np.sort(neigh_dist, axis=0)

#  for each iteration ensure the distance are set to (n_neighbors-1) k_dist = sort_neigh_dist[:, (n_neighbors-1) ]
k_dist = sort_neigh_dist[:, 6]
plt.plot(k_dist)
plt.axhline(y=0.2, linewidth=1, linestyle='dashed', color='k')
plt.ylabel("k-NN distance")
plt.xlabel("Sorted observations (6th NN)")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
clusters = DBSCAN(eps=0.2, min_samples=10).fit(df_lat_long)
# get cluster labels
clusters.labels_
# output
print(clusters.labels_)

#https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py

core_samples_mask = np.zeros_like(clusters.labels_, dtype=bool)
core_samples_mask[clusters.core_sample_indices_] = True
labels = clusters.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print("Estimated number of clusters: %d" % n_clusters_)
print("Estimated number of noise points: %d" % n_noise_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(clusters.labels_, labels))
print("Completeness: %0.3f" % metrics.completeness_score(clusters.labels_, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(clusters.labels_, labels))
print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(clusters.labels_, labels))
print(
    "Adjusted Mutual Information: %0.3f"
#     % metrics.adjusted_mutual_info_score(clusters.labels_, labels)
)
print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(df_lat_long, labels))

from collections import Counter
Counter(clusters.labels_)

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# Count the occurrences of each cluster label
cluster_counts = Counter(clusters.labels_)

# Create a bar chart using seaborn
plt.figure(figsize=(8, 6))
sns.countplot(x=clusters.labels_, palette="deep")

# Set plot labels and title
plt.xlabel("Cluster Label")
plt.ylabel("Number of Data Points")
plt.title("Distribution of Data Points Across Clusters")

# Add count labels on top of each bar
for index, value in enumerate(cluster_counts.values()):
    plt.text(index, value, str(value), ha='center', va='bottom')

# Display the plot
plt.show()

plt.figure(figsize=(5,5))
p =sns.scatterplot(data=df_lat_long, x="HARM_EV_ZSCORE", y="REL_ROAD_ZSCORE", hue=clusters.labels_, legend="full", palette="deep")
sns.move_legend(p, "upper right", bbox_to_anchor=(1.17, 1.2), title='Clusters')

plt.show()
plt.figure(figsize=(5,5))
plt.scatter(df['HARM_EV_ZSCORE'],df['REL_ROAD_ZSCORE'])

"""**EXTRA CREDIT Section:  Use the link provided to develop the best hyperparameters for your State's Clustering**"""

# Defining the list of hyperparameters to try
#https://thinkingneuron.com/how-to-create-clusters-using-dbscan-in-python/#:~:text=Finding%20Best%20hyperparameters%20for%20DBSCAN%20using%20Silhouette%20Coefficient,is%20%28b%20%E2%80%93%20a%29%20%2F%20max%20%28a%2C%20b%29.
eps_list=np.arange(start=0.1, stop=0.9, step=0.01)
min_sample_list=np.arange(start=2, stop=5, step=1)

# Creating empty data frame to store the silhouette scores for each trials
silhouette_scores_data=pd.DataFrame()

for eps_trial in eps_list:
    for min_sample_trial in min_sample_list:

        # Generating DBSAN clusters
        db = DBSCAN(eps=eps_trial, min_samples=min_sample_trial)

        if(len(np.unique(db.fit_predict(df_lat_long)))<1):
            sil_score=silhouette_score(df_lat_long, db.fit_predict(X))
        else:
            continue
        trial_parameters="eps:" + str(eps_trial.round(1)) +" min_sample :" + str(min_sample_trial)

        silhouette_scores_data=silhouette_scores_data.append(pd.DataFrame(data=[[sil_score,trial_parameters]], columns=["score", "parameters"]))
print(silhouette_scores_data)
# Finding out the best hyperparameters with highest Score
#silhouette_scores_data.sort_values(by=score, ascending=False).head(1)