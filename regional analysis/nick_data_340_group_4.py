# -*- coding: utf-8 -*-
"""NICK DATA 340 Group 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kkunedFHHugMSQbNAHj-EYeaKN9uwDpg

## Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib
from matplotlib import pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

"""## Mount Google Drive

## Import .xlsx file
"""

## New Code

# File path to the uploaded Excel file in Colab's /content directory
file_path = '/content/Region 4 Data.xlsx'

# Load the Excel file
accident_data = pd.read_excel(file_path)

# Display the first few rows of the DataFrame
accident_data.head()

accident_data.describe() # only show numerical, caterical not show

accident_data.describe(include='all') # show all columns, including numerical & categorical

print(accident_data.shape) # show the number of entries in the dataset

print(accident_data.nunique()) # total number of products & unque values of the columns(can be used for determining catergocial variables with unique value of 2)

# Categorical columns plot
accident_data['VE_TOTAL'].value_counts().plot.bar(title='Total Vehicles Involved')

sns.histplot(accident_data.FATALS, kde=True)

sns.histplot(accident_data.PERSONS, kde=True)

# plot the univariate distribution of the numerical columns
col_names = ['MONTH','DAY','DAY_WEEK', 'HOUR', 'HOURNAME', 'MINUTE' ]

fig, ax = plt.subplots(len(col_names), figsize=(6,20))

for i, col_val in enumerate(col_names):

    sns.histplot(accident_data[col_val], ax=ax[i],)
    #ax[i].set_title('Freq dist '+col_val, fontsize=10)
    #ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()
#We can see that leaving the ReleaseYear column every other column is skewed to the left
#which indicates most of the values lie in the lower range values and vice versa in the case of a ReleaseYear attribute.

accident_data

#Bivariate distribution plots

accident_hist = accident_data[['HOUR','MONTH','WEATHER','HARM_EV','FATALS','REL_ROAD' ]]

sns.pairplot(accident_hist)
#We often look out for scatter plots that follow a clear linear pattern with
#an either increasing or decreasing slope so that we can draw conclusions,
#but don’t notice these patterns in this particular dataset.
#That said, there’s always room to derive other insights that might be useful by comparing the nature of the plots between the variables of interest.

"""Missing values"""

accident_data.isnull().values.any()
 #If the above snippet returns true then there are null values in the dataset and false means there are none

accident_data.isnull().sum()
#The above snippet returns the total number of missing values across different columns

df= accident_data.copy()

"""fill nulls with zero"""

df['HOSP_HR'].fillna(0, inplace=True)
df['HOSP_HRNAME'].fillna(0, inplace=True)

df.isnull().values.any()

## # Drop rows with missing values
df.dropna(axis=0)

# Drop columns with missing values
df.dropna(axis=1)

# Drop the columns where all elements are missing values:
df.dropna(axis=1, how='all')

# Drop the columns where any of the elements are missing values
df.dropna(axis=1, how='any')

# Keep only the rows which contain 2 missing values maximum
df.dropna(thresh=2)

# Drop the columns where any of the elements are missing values
df.dropna(axis=1, how='any')

# Fill all missing values with the mean of the particular column
#df.fillna(df.mean())

# Fill any missing value in column 'PERSONS' with the column median
df['PERSONS'].fillna(df['PERSONS'].median())

# Fill any missing value in column 'Depeche' with the column mode
df['VE_TOTAL'].fillna(df['VE_TOTAL'].mode())

# interpolate() function will perform a linear interpolation at the missing data points
# to “guess” the value that is most likely to be filled in.
df.interpolate()

"""outlier detection"""

col_names = ['LONGITUD','MONTH','WEATHER','HARM_EV','LATITUDE','REL_ROAD'  ]

fig, ax = plt.subplots(len(col_names), figsize=(8,30))

for i, col_val in enumerate(col_names):

    sns.boxplot(y=accident_data[col_val], ax=ax[i])
    ax[i].set_title('Box plot - {}'.format(col_val), fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)

plt.show()
#we identify outliers the black dots are outliers in the strength factor attribute and the red colored box is the IQR range.

"""outlier removal"""

def percentile_based_outlier(data, threshold=95):
    diff = (100 - threshold) / 2
    minval, maxval = np.percentile(data, [diff, 100 - diff])
    return (data < minval) | (data > maxval)

col_names = ['LONGITUD','MONTH','WEATHER','HARM_EV','LATITUDE','REL_ROAD'  ]

fig, ax = plt.subplots(len(col_names), figsize=(8,40))

for i, col_val in enumerate(col_names):
    x = accident_data[col_val][:1000]
    sns.distplot(x, ax=ax[i], rug=True, hist=False)
    outliers = x[percentile_based_outlier(x)]s
    ax[i].plot(outliers, np.zeros_like(outliers), 'ro', clip_on=False)

    ax[i].set_title('Outlier detection - {}'.format(col_val), fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)

plt.show()

"""Correlation matrix"""

f, ax = plt.subplots(figsize=(10, 8))
corr = accident_data[['LONGITUD','MONTH','WEATHER','HARM_EV','LATITUDE','REL_ROAD' ]].corr()
sns.heatmap(corr,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)

"""# KNN

"""

plt.figure(figsize=(7,7))
plt.scatter(df['DAY'], df['MONTH'],s=df['FATALS'],c=df['FATALS'])

features=list(zip(df['DAY'], df['MONTH']))
##features DAY 9/31,MONTH 7/12

label=list(df['FATALS'])
##label

model = KNeighborsClassifier(n_neighbors=3) # select k=2

# Train the model using the training sets
model.fit(features,label)

#Predict Output
predicted= model.predict([[6,12]]) # 6:SATURDAY, 12:DECEMBER
print(predicted)

"""Split data"""

# Import train_test_split function
from sklearn.model_selection import train_test_split

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.85) # 70% training and 30% test

"""K=1"""

#Import knearest neighbors Classifier model
from sklearn.neighbors import KNeighborsClassifier

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=1)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

"""evaluate K=1"""

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""k=3"""

#Import knearest neighbors Classifier model
from sklearn.neighbors import KNeighborsClassifier

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=3)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

"""evaluate k = 3"""

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Commented out IPython magic to ensure Python compatibility.
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
# %matplotlib inline

numNeighbors = [1,2,3,4,5,6,7,8,9,10]
trainAcc = []
testAcc = []

for k in numNeighbors:
    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)
    clf.fit(X_train, y_train)
    Y_predTrain = clf.predict(X_train)
    Y_predTest = clf.predict(X_test)
    trainAcc.append(accuracy_score(y_train, Y_predTrain))
    testAcc.append(accuracy_score(y_test, Y_predTest))

plt.plot(numNeighbors, trainAcc, 'ro-', numNeighbors, testAcc,'bv--')
plt.legend(['Training Accuracy','Test Accuracy'])
plt.xlabel('Number of neighbors')
plt.ylabel('Accuracy')

"""Group 2 Vehicles/Collisions VE_TOTAL, HARM_EV, MAN_COLL, LGT_COND"""

plt.figure(figsize=(7,7))
plt.scatter(df['VE_TOTAL'], df['HARM_EV'],s=df['FATALS'],c=df['FATALS'])

##plt.scatter(df['VE_TOTAL'], df['HARM_EV'], df['MAN_COLL'], df['LGT_COND'])

plt.scatter(df['PERSONS'], df['WEATHER1'],s=df['FATALS'],c=df['FATALS'])

features=list(zip(df['VE_TOTAL'], df['HARM_EV'], df['PERSONS'], df['WEATHER1']))
##features

label=list(df['FATALS'])
##label

model = KNeighborsClassifier(n_neighbors=3) # select k=2

# Train the model using the training sets
model.fit(features,label)

#Predict Output
predicted= model.predict([[3,1,2,4]]) # 6:SATURDAY, 12:DECEMBER
print(predicted)

"""SPLIT DATA"""

# Import train_test_split function
from sklearn.model_selection import train_test_split

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.85) # 70% training and 30% test

"""k = 1"""

#Import knearest neighbors Classifier model
from sklearn.neighbors import KNeighborsClassifier

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=1)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

"""evaluate k=1"""

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""k=3"""

#Import knearest neighbors Classifier model
from sklearn.neighbors import KNeighborsClassifier

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=3)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

"""evaluate k=3"""

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Commented out IPython magic to ensure Python compatibility.
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
# %matplotlib inline

numNeighbors = [1,2,3,4,5,6,7,8,9,10]
trainAcc = []
testAcc = []

for k in numNeighbors:
    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)
    clf.fit(X_train, y_train)
    Y_predTrain = clf.predict(X_train)
    Y_predTest = clf.predict(X_test)
    trainAcc.append(accuracy_score(y_train, Y_predTrain))
    testAcc.append(accuracy_score(y_test, Y_predTest))

plt.plot(numNeighbors, trainAcc, 'ro-', numNeighbors, testAcc,'bv--')
plt.legend(['Training Accuracy','Test Accuracy'])
plt.xlabel('Number of neighbors')
plt.ylabel('Accuracy')